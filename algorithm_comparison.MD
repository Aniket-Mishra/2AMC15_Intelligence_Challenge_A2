### Comparison of Deep RL Algorithms for 2D Delivery Robot (Continuous State, Discrete Action)

| **Algorithm**   | **Core Formula / Loss**                                                                                 | **Type**                    | **Action Space**            | **Pros**                                                                                       | **Cons**                                                                                      | **Handles Unknown Goal?**        | **Training Stability**     | **Sample Efficiency**       | **Exploration Capability**     |
|----------------|----------------------------------------------------------------------------------------------------------|-----------------------------|-----------------------------|------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|----------------------------------|-----------------------------|-----------------------------|-------------------------------|
| **Dueling DQN** | Q(s, a) = V(s) + A(s, a) - avg(A(s, a′))                                                                 | Value-based (off-policy)    | Discrete only               | 1. Improved value estimation  <br> Simple upgrade from normal DQN <br> 2. Strong baseline and easier to implement is what I understood               | 1. Struggles with partial observability - Apparently this is a biggie <br> 2. Needs tuned ε-greedy                            | Limited without reward shaping    | Stable (with Double DQN)    | High (via replay buffer)    | Weak (unless using NoisyNets) - Chatgpt told me this |
| **PPO**         | L^CLIP = E[min(rₜ Â, clip(rₜ, 1±ε) Â)]                                                                   | Policy-gradient (on-policy) | Discrete & Continuous - This would be good cuz it has both      | 1. Very stable - famous <br> 2. Robust to bad rewards - Which is nice <br> 3. Supports parallelism - Which is lovely tbh <br> 4. Good in POMDPs - Need more yt vids on this  | 1. Less sample efficient <br> 2. On-policy only                                                | Excellent                          | Very stable                 | Moderate (can parallelize)   | Strong (entropy bonus)         |
| **A3C**         | Actor: ∇ log π(a|s) Â <br> Critic: (R − V(s))²                                                           | Policy-gradient (on-policy) | 1. Discrete & Continuous - Again good for us      | 2. Fast training via workers - So llel <br> 3. Good exploration <br> 4. Easy to implement (NICE) <3                | 1. Less stable than PPO (Multiple proofs it seems) <br> 2. Outdated compared to PPO - Old famous kid                                       | Good                               | Moderate                    | Medium (no replay buffer)    | Strong                          |
| **DDPG**        | Critic: (Q(s, a) − (r + γ Q′(s′, a′)))² <br> Actor: maximize Q(s, μ(s))                                  | Actor-critic (off-policy)   | Continuous only             | 1. High control fidelity <br> 2. Off-policy efficient <br> 3. Used in robotics - <3                  | 1. Not for discrete (So we should skip?) actions <br> 2. Sensitive to noise/tuning <br> 3. Poor in sparse rewards     | Not suitable                        | Can be unstable             | High (via replay)            | Weak (needs noise injection)   |
| **SAC**         | J = E[Q(s,a) − α log π(a|s)]                                                                             | Actor-critic (off-policy)   | Discrete (with extension)   | 1. Very stable <br> 2. Best exploration (via entropy) <br> 3. Strong performance                 | 1. Needs adaptation for discrete (Do we wanna spend the time?)<br> 2. Fewer implementations (Not many examples either) <br> 3. Complex (Yeah, wth is with this)                  | Excellent (entropy-driven)        | Very stable - Fine Fine                | Very high                    | Excellent                        |

---

### Chosen:

- Supports discrete actions -> up down left right
- Stochastic policy: enables better exploration -> Since we don't know target, helps search better
- Stable training due to clipping mechanism -> Lots of yt videos talk about this too
- works well in robot navigation tasks with unknown or dynamic goals
