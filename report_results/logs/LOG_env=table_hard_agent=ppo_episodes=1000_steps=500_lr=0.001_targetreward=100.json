{
  "agent": "ppo",
  "agent_config": {
    "state_dim": 3,
    "action_dim": 3,
    "gamma": 0.99,
    "clip_epsilon": 0.1,
    "update_epochs": 10,
    "batch_size": 64,
    "gae_lambda": 0.95,
    "lr": 0.001
  },
  "timestamp": "20250624_174722",
  "episodes": 1000,
  "max_steps": 500,
  "seed": 42,
  "success_rate": 0.918,
  "avg_reward_last_50": -42.701999999999664,
  "final_eval_steps": 84,
  "reward_file": "results_common/rewards/METRICS_env=table_hard_agent=ppo_episodes=1000_steps=500_lr=0.001_targetreward=100.csv",
  "path_file": "results_common/graph_path/GRAPH_env=table_hard_agent=ppo_episodes=1000_steps=500_lr=0.001_targetreward=100.npy",
  "reward_plot_png": "results_common/graphs/METRICS_env=table_hard_agent=ppo_episodes=1000_steps=500_lr=0.001_targetreward=100.png",
  "path_plot_png": "results\\PATH_env=table_hard_agent=ppo_episodes=1000_steps=500_lr=0.001_targetreward=100.png",
  "EVALUATION STEPS": 85,
  "CONVERGENCE EPISODE": 957
}